import java.util.Properties
import scala.io.Source
import org.apache.kafka.clients.producer.{KafkaProducer, ProducerRecord}
import org.apache.spark.sql.SparkSession
import org.apache.spark.sql.types.{IntegerType, StringType, StructType}
import org.apache.spark.sql.functions.{from_json, col}

val spark: SparkSession = SparkSession.builder().master("local[2]").appName("Proyecto-SparkScala")
    .getOrCreate()

// Productor
val props:Properties = new Properties()
props.put("bootstrap.servers","localhost:9092")
props.put("key.serializer","org.apache.kafka.common.serialization.StringSerializer")
props.put("value.serializer","org.apache.kafka.common.serialization.StringSerializer")
props.put("acks","all")

val productor = new KafkaProducer[String,String](props)

val topic="projectBigDataProcessing"
val json = Source.fromFile("/home/inot/personal.json").getLines.mkString.replace("\n", "").replace(" ", "").replace("},", "},\n").split("\n")
// Formateo el string en base a la estructura del texto para que se env√≠en los registros de uno en uno, sin espacios raros, y que el consumer pueda reproducir y procesar el json correctamente.
for (person <- json) productor.send(new ProducerRecord[String,String](topic, person))
    
productor.close()

// Consumidor
val df = spark.readStream
    .format("kafka")
    .option("kafka.bootstrap.servers","localhost:9092")
    .option("subscribe","projectBigDataProcessing")
    .option("startingOffsets","earliest")
    .load()

val res = df.selectExpr("CAST(value AS STRING)")
val schema = new StructType()
    .add("id",IntegerType)
    .add("first_name",StringType)
    .add("last_name",StringType)
    .add("email",StringType)
    .add("gender",StringType)
    .add("ip_address",StringType)

val personal = res.select(from_json(col("value"),schema).as("data"))
    .select("data.*").where("first_name != 'Noell'")

personal.writeStream
    .format("console")
    .outputMode("append")
    .start()
    .awaitTermination()
